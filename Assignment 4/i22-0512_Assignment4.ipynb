{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd097a40",
   "metadata": {
    "id": "fd097a40"
   },
   "source": [
    "# Question1\n",
    " POETRY Generation using N-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0a929af",
   "metadata": {
    "id": "bbfd8420"
   },
   "source": [
    "### 1 Introduction:\n",
    "In this assignment, you will use n-gram language modeling to generate some poetry using the ngrams. For the purpose of this assignment a poem will consist of three stanzas each containing four verses where each verse consists of 7—10 words. For example, following is a manually generated stanza.\n",
    "\n",
    "دل سے نکال یاس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "ہوتا ہے کیوں اداس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "مایوسیوں کی قید سے خود کو نکال کر،\n",
    "\n",
    "آ جاؤ میرے پاس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "\n",
    "\n",
    "آ کر کبھی تو دید سے سیراب کر مجھے،\n",
    "\n",
    "مرتی نہیں ہے پیاس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "مہر و وفا خلوص و محبت گداز دل،\n",
    "\n",
    "سب کچھ ہے میرے پاس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "لوٹیں گے تیرے آتے ہی پھر دن بہار کے،\n",
    "\n",
    "رہتی ہے دل میں آس کہ زندہ ہوں میں،\n",
    "\n",
    "نایا ب شاخ چشم میں کھلتے ہیں اب بھی خواب، سچ ہے ترا\n",
    "\n",
    "قیاس کہ زندہ ہوں میں ابھی\n",
    "\n",
    "The task is to print three such stanzas with an empty line in between. The generation model can be trained on the provided Poetry Corpus containing poems from Faiz, Ghalib and Iqbal.You can scrape other urdu poetry too from internet. You will train unigram and bigram models using this corpus. These models will be used to generate poetry.\n",
    "\n",
    "2 Assignment Task:\n",
    "\n",
    "The task is to generate a poem using different models. We will generate a poem verse by verse until all stanzas have been generated. The poetry generation problem can be solved using the following algorithm:\n",
    "1. Load the Poetry Corpus\n",
    "2. Tokenize the corpus in order to split it into a list of words\n",
    "3. Generate n-gram models\n",
    "4. For each of the stanzas\n",
    "– For each verse\n",
    "* Generate a random number in the range [7...10]\n",
    "* Select first word\n",
    "* Select subsequent words until end of verse\n",
    "* [bonus] If not the first verse, try to rhyme the last word with the last word of the previous verse\n",
    "* Print verse\n",
    "– Print empty line after stanza\n",
    "2.1 Implementation Challenges:\n",
    "\n",
    "Among the challenges of solving this assignment will be selecting subsequent words once we have chosen the first word of the verse. To predict the next word, what we aim to compute is the most probable next word from all the possible next words. In other words, we need to find the set of words that occur most frequently after the already selected word and choose the next word from that set. We can use a Conditional Frequency Distribution (CFD) to figure that out! A CFD tells us: given a condition, what is likelihood of each possible outcome. [bonus] Rhyming the generated verses is also a challenge. You can build your dictionary for rhyming. The Urdu sentence is written from right to left, so makes your n-gram models according to this style.\n",
    "\n",
    "2.2 Standard n-gram Models\n",
    "We can develop our model using the Conditional Frequency Distribution method. First develop a unigram model (Unigram Model), then the bigram model (Bigram Model) and then trigram model. Select the first word of each line randomly from starting words in the vocabulary and then use the bigram model to generate the next word until the verse is complete. Generate the next three lines similarly.\n",
    " Follow the same steps for the trigram model and compare the results of the two n-gram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a243468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize, sent_tokenize \n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52afe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ghafile = open('ghalib.txt',encoding='utf-8')\n",
    "iqfile = open('iqbal.txt',encoding='utf-8')\n",
    "\n",
    "ghastr = ghafile.read()                                    #ghalib string\n",
    "iqstr = iqfile.read()                                #iqbal string\\\n",
    "combstr = ghastr+iqstr\n",
    "\n",
    "tokens = word_tokenize(combstr)                 # makes tokens of the combined string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb709115",
   "metadata": {
    "id": "bb709115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! جس کو دیکھتے ہی ہے تو نے گا کبھی جو عہد ⹁\n",
      "وہ گل سے نہ ہے وہ گل ہیں ہوں ایک بار ⹁\n",
      "اب میں نے گا سارا جہان بے ذوق نہیں ھوں کہ ⹁\n",
      "کی ہے وہ خاک بازی عشق نے چاہا اسے شیوہ ہائے ⹁\n",
      "کو دیکھتے ہی اُس کے ساتھ سونے کا کیا خوب کلام ⹁\n",
      "حجاب میں بھی ہے وہ دل و بھی جو حجاب کا ⹁\n",
      "ہو ہے یا آیا دھمکی میں بھی نہیں ہے تو پھر ⹁\n",
      "آنے کی ہے کہ ’ یاد ہے وہ اب فرد جو ⹁\n",
      "وائے سرِ پرشور بیخوابی سے دل میں نے چاہا تھا وہ ⹁\n",
      "اے دل میں بھی ہیں ہوں کہ ’ یاد آیا کہ ⹁\n",
      "سے گزر میں بھی جو حجاب کوئی اس ! دل کی ⹁\n",
      "آیا تغافل نہ ہوا ہوں مگر نیا میں ہے وہ گل ⹁\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "### For BI-grams\n",
    "rand_len  = random.randint(7,10)\n",
    "# print(rand_len , \"\\n\")\n",
    "starting_word = tokens[random.randint(0,len(combstr.split())-1)]              # starting word of misra\n",
    "linelen = 12\n",
    "frstwordchk = True\n",
    "generstanza = ''                               #generated stanza\n",
    "ll=0\n",
    "\n",
    "while ll < linelen:\n",
    "    starting_word = tokens[random.randint(0,len(combstr.split())-1)]              # starting word of misra\n",
    "    linstr = ''\n",
    "    currword = starting_word\n",
    "\n",
    "    if frstwordchk == True:\n",
    "        currword = starting_word\n",
    "        linstr+= currword\n",
    "        linstr+=\" \"\n",
    "        frstwordchk = False\n",
    "        \n",
    "    for i in range(rand_len+1):\n",
    "        list_to_store_matching_words = []\n",
    "        bigrams = nltk.ngrams(tokens,2)             #generating bigrams \n",
    "        bgfd = nltk.FreqDist(bigrams)              ## bigrams frequency distribution\n",
    "        bglist = list(bgfd)                       ### converting to list so we can check for values\n",
    "        for j in bglist:\n",
    "            if currword in j[0] and \"‘\" not in j:          # it will check if first word in bigrams is equal to curr word that is appended to the string.\n",
    "                list_to_store_matching_words.append(j)\n",
    "                nbgfd = nltk.FreqDist(list_to_store_matching_words)\n",
    "                \n",
    "        currword = nbgfd.most_common(5)               # this outputs a tuple with freq and another tuple of bigram \n",
    "#         print(currword)\n",
    "        minlength_of_the_bigram_array = len(currword)\n",
    "#         print(minlength_of_the_bigram_array  , \"IS the min length\")\n",
    "        ####### Below line prints selects one of the birgrams randomly from a group of bigrams and then after selecting that it selects The second word in those bigrams\n",
    "        currword = currword[random.randint(0,minlength_of_the_bigram_array-1)][0][1]                   # this is the second value of the bigram to be put in the misra\n",
    "        linstr+=currword                              ## Concatinating the word to the misra string\n",
    "        linstr+=\" \"\n",
    "        if (i == rand_len):\n",
    "            linstr+='⹁\\n'\n",
    "        \n",
    "#     print(linstr)\n",
    "    generstanza+= linstr\n",
    "#     print(linstr)\n",
    "#     break\n",
    "    \n",
    "    ll+=1\n",
    "\n",
    "bigrams = nltk.ngrams(tokens,2)              #generating bigrams \n",
    "bgfd = nltk.FreqDist(bigrams)\n",
    "# bgfd.most_common(50)\n",
    "print(generstanza)\n",
    "\n",
    "#### date 19/10 currently the phrase is being repeated for the next line\n",
    "## Works fine date 21/10\n",
    "# the Commented lines other than actual comments are for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ab5ecb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 \n",
      "\n",
      "پرنم اسد خستہ جاں دادہٴ سرِ پرشور سے جس سمجھ ہو ⹁\n",
      "کر تھا زندگی میں ایک موت ہے تیری بے سرور ⹁\n",
      "تھا دل درد تھا زندگی پھ خار جستجو مدعا ہے ⹁\n",
      "رہتے یہی انتظار گرچہ جاں کا لگا منہ چھپائے دشنۂ ⹁\n",
      "کیا مرا پیش بھی اگر ہے تو ہوتا تھا دل ⹁\n",
      "ہے یا ہے اس ہوگا سکوت ساز تو ہوتا تھا ⹁\n",
      "سمجھ کبريا ہے اس کی ! آ گرد میں دونوں ⹁\n",
      "ساقی کا عام ہے اس نکل تصویر کاو کا ہوا ⹁\n",
      "رکھ ترا سرکش دبا ہے اسد سینہ، تھا جاتی ہیں ⹁\n",
      "موجِ دشتِ دلِ پُر رکھتے تو ہوتا اور جیتے رہتے ⹁\n",
      "سکندری ہے میرا مومن خود ہشیاری کو بادہ خوار ہوتا ⹁\n",
      "گردش میری گر گیا تھا میں ہو تھا نسخہ وفا ⹁\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "### For TRI-grams\n",
    "rand_len  = random.randint(7,10)\n",
    "print(rand_len , \"\\n\")\n",
    "starting_word = tokens[random.randint(0,len(combstr.split())-1)]              # starting word of misra\n",
    "linelen = 12\n",
    "frstwordchk = True\n",
    "generstanza = ''                               #generated stanza\n",
    "ll=0\n",
    "\n",
    "while ll < linelen:\n",
    "    starting_word = tokens[random.randint(0,len(combstr.split())-1)]              # starting word of misra\n",
    "    linstr = ''\n",
    "    currword = starting_word\n",
    "\n",
    "    if frstwordchk == True:\n",
    "        currword = starting_word\n",
    "        linstr+= currword\n",
    "        linstr+=\" \"\n",
    "        frstwordchk = False\n",
    "        \n",
    "    for i in range(rand_len+1):\n",
    "        list_to_store_matching_words = []\n",
    "        trigrams = nltk.ngrams(tokens,3)             #generating bigrams \n",
    "        trfd = nltk.FreqDist(trigrams)              ## bigrams frequency distribution\n",
    "        trlist = list(trfd)                       ### converting to list so we can check for values\n",
    "        for j in trlist:\n",
    "            if currword in j[0] and \"‘\" not in j:          # it will check if first word in bigrams is equal to curr word that is appended to the string.\n",
    "                list_to_store_matching_words.append(j)\n",
    "                ntrfd = nltk.FreqDist(list_to_store_matching_words)\n",
    "                \n",
    "        currword = ntrfd.most_common(5)               # this outputs a tuple with freq and another tuple of bigram \n",
    "#         print(currword)\n",
    "        minlength_of_the_trigram_array = len(currword)\n",
    "#         print(minlength_of_the_bigram_array  , \"IS the min length\")\n",
    "        ####### Below line prints selects one of the birgrams randomly from a group of bigrams and then after selecting that it selects The second word in those bigrams\n",
    "        currword = currword[random.randint(0,minlength_of_the_trigram_array-1)][0][random.randint(1,2)]                   # this is the second value of the bigram to be put in the misra\n",
    "        \n",
    "        linstr+=currword                              ## Concatinating the word to the misra string\n",
    "        linstr+=\" \"\n",
    "        if (i == rand_len):\n",
    "            linstr+='⹁\\n'\n",
    "        \n",
    "#     print(linstr)\n",
    "    generstanza+= linstr\n",
    "#     print(linstr)\n",
    "#     break\n",
    "    ll+=1\n",
    "\n",
    "# bigrams = nltk.ngrams(tokens,2)              #generating bigrams \n",
    "# bgfd = nltk.FreqDist(bigrams)\n",
    "# bgfd.most_common(50)\n",
    "print(generstanza)\n",
    "\n",
    "#### date 19/10 currently the phrase is being repeated for the next line\n",
    "## Works fine date 21/10\n",
    "# the Commented lines other than actual comments are for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7dffb1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# li = [i for currword in list(bigrams)]\n",
    "# bigrams = nltk.ngrams(tokens,2)              #generating bigrams \n",
    "# bgfd = nltk.FreqDist(bigrams)                                                       ### bigrams frequency distribution\n",
    "# li = list(bgfd)\n",
    "# starting_word = tokens[random.randint(0,len(combstr.split()))]              # starting word of misra\n",
    "# lis = []\n",
    "# # starting_word = 'نذرانےنہ'\n",
    "# for i in li:\n",
    "# #     starting_word = tokens[random.randint(0,len(combstr.split()))]              # starting word of misra\n",
    "#     if starting_word in i[0] and \"‘\" not in i:\n",
    "# #         print(i)\n",
    "# #         print(i[1])\n",
    "#         lis.append(i)\n",
    "#         nbgfd = nltk.FreqDist(lis)\n",
    "# # print(starting_word)\n",
    "# print(nbgfd.most_common(3))\n",
    "# '''Testing for the code written above'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79601988",
   "metadata": {
    "id": "79601988"
   },
   "source": [
    "# Question2\n",
    " Classify language out of the list given below using just stop words. Remove punctuations, make it lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa286b84",
   "metadata": {
    "id": "fa286b84",
    "outputId": "eed1da3f-fabe-4ef1-ba32-dbf482163b85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bdad25b",
   "metadata": {
    "id": "1bdad25b"
   },
   "outputs": [],
   "source": [
    "Test=\"An article is qualunque member van un class of dedicated words naquele estão used with noun phrases per mark the identifiability of the referents of the noun phrases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266654b6",
   "metadata": {
    "id": "266654b6",
    "outputId": "38cd33e4-19c0-4338-a6af-6b2951888fe9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arabic': 0,\n",
       " 'azerbaijani': 1,\n",
       " 'basque': 0,\n",
       " 'bengali': 0,\n",
       " 'catalan': 3,\n",
       " 'chinese': 0,\n",
       " 'danish': 0,\n",
       " 'dutch': 3,\n",
       " 'english': 5,\n",
       " 'finnish': 0,\n",
       " 'french': 1,\n",
       " 'german': 1,\n",
       " 'greek': 0,\n",
       " 'hebrew': 0,\n",
       " 'hinglish': 8,\n",
       " 'hungarian': 1,\n",
       " 'indonesian': 1,\n",
       " 'italian': 2,\n",
       " 'kazakh': 0,\n",
       " 'nepali': 0,\n",
       " 'norwegian': 0,\n",
       " 'portuguese': 1,\n",
       " 'romanian': 1,\n",
       " 'russian': 0,\n",
       " 'slovene': 0,\n",
       " 'spanish': 1,\n",
       " 'swedish': 0,\n",
       " 'tajik': 0,\n",
       " 'turkish': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your output looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d15f2698",
   "metadata": {
    "id": "d15f2698"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d945169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic 0\n",
      "azerbaijani 1\n",
      "basque 0\n",
      "bengali 0\n",
      "catalan 3\n",
      "chinese 0\n",
      "danish 0\n",
      "dutch 3\n",
      "english 5\n",
      "finnish 0\n",
      "french 1\n",
      "german 1\n",
      "greek 0\n",
      "hebrew 0\n",
      "hinglish 8\n",
      "hungarian 1\n",
      "indonesian 1\n",
      "italian 2\n",
      "kazakh 0\n",
      "nepali 0\n",
      "norwegian 0\n",
      "portuguese 1\n",
      "romanian 1\n",
      "russian 0\n",
      "slovene 0\n",
      "spanish 1\n",
      "swedish 0\n",
      "tajik 0\n",
      "turkish 0\n"
     ]
    }
   ],
   "source": [
    "langarr = stopwords.fileids() \n",
    "test = Test.lower()\n",
    "\n",
    "tokens = word_tokenize(test)\n",
    "langdict = {'arabic': [],\n",
    " 'azerbaijani':[],\n",
    " 'basque':[],\n",
    " 'bengali':[],\n",
    " 'catalan':[],\n",
    " 'chinese':[],\n",
    " 'danish':[],\n",
    " 'dutch':[],\n",
    " 'english':[],\n",
    " 'finnish':[],\n",
    " 'french':[],\n",
    " 'german':[],\n",
    " 'greek':[],\n",
    " 'hebrew':[],\n",
    " 'hinglish':[],\n",
    " 'hungarian':[],\n",
    " 'indonesian':[],\n",
    " 'italian':[],\n",
    " 'kazakh':[],\n",
    " 'nepali':[],\n",
    " 'norwegian':[],\n",
    " 'portuguese':[],\n",
    " 'romanian':[],\n",
    " 'russian':[],\n",
    " 'slovene':[],\n",
    " 'spanish':[],\n",
    " 'swedish':[],\n",
    " 'tajik':[],\n",
    " 'turkish':[] }\n",
    "\n",
    "langar = []\n",
    "for languages in langarr:\n",
    "    stp_words = set(stopwords.words(languages))\n",
    "    langdict[languages] = stp_words\n",
    "\n",
    "freqArr = []\n",
    "\n",
    "for lang in langdict.keys():\n",
    "    rep_words = []\n",
    "    count = 0\n",
    "    for token in tokens:                   \n",
    "        if token in langdict[lang]:                      ## this is to check if the token is present in the languages dictionary\n",
    "            if token not in rep_words:                   ## and not in the repeated words \n",
    "                count+=1                                 ## increments the langauge count\n",
    "        rep_words.append(token)                          ## appends the occurred word to an array\n",
    "    freqArr.append(count)                                ## appends the corresponding frequncy to the array\n",
    "for i in range(len(langdict.keys())):\n",
    "    print(langarr[i], freqArr[i])                        ## shows frequency along with the language in which it occured\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43708fa5",
   "metadata": {
    "id": "43708fa5"
   },
   "source": [
    "# Question 3\n",
    " Rule Based Roman Urdu Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f761e",
   "metadata": {
    "id": "2e1f761e"
   },
   "source": [
    "Roman Urdu lacks standard lexicon and usually many spelling variations exist for a given word, e.g., the word zindagi (life) is also written as zindagee, zindagy, zaindagee and zndagi. So, in this question you have to Normalize Roman Urdu words using the following Rules given in the attached Pdf. Your Code works for a complete Sentence or multiple sentences.\n",
    "\n",
    "For Example: zaroori, zaruri, zarori map to the 'zrory'. So zrory becomes the correct word for all representations mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db4024da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd7e7159",
   "metadata": {
    "id": "dd7e7159"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Rule 1  : meeera naam Ali hai\n",
      " Rule 2  : meeera naam Ali hai\n",
      " Rule 3  : meeera naam Ali hae\n",
      " Rule 4  : meeera naam Ali hae\n",
      " Rule 5  : meeera naam Ali hae\n",
      " Rule 6  : meeera naam Ali hae\n",
      " Rule 7  : meeera naam Ali hae \n",
      " Rule 8  : meeera naam Ali hae\n",
      " Rule 9  : meeera naam Ali hae\n",
      " Rule 10 : meeera naam Ali hae\n",
      " Rule 11 : meeera naam Ali hae\n",
      " Rule 12 : meeera naam Ali hae\n",
      " Rule 13 : meeera nam Ali hae\n",
      " Rule 14 : meeera nam Ali hae \n",
      " Rule 15 : meeera nam Ali hae\n",
      " Rule 16 : meeera nam Ali hae\n",
      " Rule 17 : miera nam Ali hae\n",
      " Rule 18 : miera nam Ali hae\n",
      " Rule 19 : miera nam Ali hae\n",
      " Rule 20 : miera nam Ali hae\n",
      " Rule 21 : miera nam Ali hae\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "tes = \"hello nigga how is the zindagi going, my name is not Taha, mera dost is not !(!Dumb), mera university bilkul bhi ragraaa nai detii, balkey dho deti hai poori tarah se\"\n",
    "# tes = \"zindagee\"\n",
    "# tes = 'aintarfaiiiinnn'\n",
    "# # tes = \"ayeeay\"\n",
    "# # tes = 'arfaarar'\n",
    "# # tes = 'alihhh'\n",
    "# # tes = 'shapprey'\n",
    "# # tes = 'hisssss'\n",
    "# # tes = 'barbie'\n",
    "# tes = 'ryumanry sum'\n",
    "# tes = 'estrumanes'\n",
    "# tes = 'sykhsyafsy'\n",
    "# tes = \"khaana khaa lo \"\n",
    "# tes = 'tyri'\n",
    "# tes = 'banjjjer'\n",
    "# tes = \"angoor khate hain\"\n",
    "# tes = 'maraci'\n",
    "tes = \"meeera naam Ali hai\"\n",
    "# tes = \"banddd karo ye sab\"\n",
    "# tes = 'delulu is the solulu'\n",
    "# tes = 'bruhhuhuh'\n",
    "\n",
    "\n",
    "wordfixr1 = re.sub(r'ain$' , 'ein', tes)          ## checks if ain is present with multiple \n",
    "wordfixr2 = re.sub(r'\\Bar', 'r' , wordfixr1)\n",
    "wordfixr3 = re.sub(r'ai', 'ae' , wordfixr2)\n",
    "wordfixr4 = re.sub(r'iy+' , 'i' , wordfixr3)\n",
    "wordfixr5 = re.sub(r'ay$' , 'e' , wordfixr4)\n",
    "wordfixr6 = re.sub(r'ih+', 'eh' , wordfixr5)\n",
    "wordfixr7 = re.sub(r'ey$' , 'e' , wordfixr6)\n",
    "wordfixr8 = re.sub(r's+', 's' , wordfixr7)\n",
    "wordfixr9 = re.sub(r'ie$' , 'y' , wordfixr8)\n",
    "wordfixr10 = re.sub(r'ry\\B' , 'ri' , wordfixr9)\n",
    "wordfixr11 = re.sub(r'^es' , 'is' , wordfixr10)\n",
    "wordfixr12 = re.sub(r'sy\\B' , 'si' , wordfixr11)\n",
    "wordfixr13 = re.sub(r'a+' ,'a' , wordfixr12)\n",
    "wordfixr14 = re.sub(r'ty\\B', 'ti' , wordfixr13)\n",
    "wordfixr15 = re.sub(r'j+', 'j', wordfixr14)\n",
    "wordfixr16 = re.sub(r'o+','o',wordfixr15)\n",
    "wordfixr17 = re.sub(r'(ee)+' , 'i' , wordfixr16)\n",
    "wordfixr18 = re.sub(r'([bcdefghijklmnopqrtuvwxyz])i$', r'\\1y',wordfixr17)\n",
    "wordfixr19 = re.sub(r'd+', 'd', wordfixr18)\n",
    "wordfixr20 = re.sub(r'u', 'o' , wordfixr19)\n",
    "wordfixr21 = re.sub(r'([acefghijlmnoqrstuvwxyz])h+', r'\\1',wordfixr20)\n",
    "\n",
    "print(f\" Rule 1  : {wordfixr1}\\n Rule 2  : {wordfixr2}\\n Rule 3  : {wordfixr3}\\n Rule 4  : {wordfixr4}\\n Rule 5  : {wordfixr5}\\n Rule 6  : {wordfixr6}\\n Rule 7  : {wordfixr7} \" )\n",
    "print(f\" Rule 8  : {wordfixr8}\\n Rule 9  : {wordfixr9}\\n Rule 10 : {wordfixr10}\\n Rule 11 : {wordfixr11}\\n Rule 12 : {wordfixr12}\\n Rule 13 : {wordfixr13}\\n Rule 14 : {wordfixr14} \")\n",
    "print(f\" Rule 15 : {wordfixr15}\\n Rule 16 : {wordfixr16}\\n Rule 17 : {wordfixr17}\\n Rule 18 : {wordfixr18}\\n Rule 19 : {wordfixr19}\\n Rule 20 : {wordfixr20}\\n Rule 21 : {wordfixr21}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24157ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "2c52dd23",
   "metadata": {
    "id": "m7iy18K4vJYt"
   },
   "source": [
    "# Question 4\n",
    "In this question, you have been given two text files in Urdu. The first file contains an Urdu dictionary,\n",
    "which consists of a list of words. The second file contains sentences that do not have spaces between the\n",
    "words and are difficult to read.\n",
    "آجخودبخشہوں\n",
    "This sentence, without proper word segmentation, is difficult to understand. However, with proper word\n",
    "segmentation, the sentence can be separated into individual words:\n",
    "آج خود بخش ہوں\n",
    "This makes the sentence much easier to read and understand.\n",
    "\n",
    "\n",
    "This task is create spaces between words using\n",
    "\n",
    "*   unigrams\n",
    "*   bigram\n",
    "*   trigrams\n",
    "\n",
    "You can use the list of words file/dictionary provided in assignment 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4fca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd055d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "14239555",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_file = open('word_test.txt', encoding='utf-8')\n",
    "dfile1 = open('Word dictionary/words.txt', encoding='utf-8')\n",
    "dfile2 = open('Word dictionary/bigram_words.txt', encoding='utf-8')\n",
    "\n",
    "teststr = \"تجربہکارہندوستانیآفسپنررویچندرنایشوننےآئندہایشیاءکپ2023ءکیغیریقینیقسمتپراپنیرائےکااظہارکیاہےجوپاکستانمیںہونےجارہاہے\"\n",
    "rightstr =\"تجربہ کار ہندوستانی آف سپنر روی چندرن ایشون نے آئندہ ایشیاء کپ 2023ء کی غیر یقینی قسمت پر اپنی رائے کا اظہار کیا ہے، جو پاکستان میں ہونے جا رہا ہے۔ \"\n",
    "wfile = wrong_file.read()\n",
    "df1 = dfile1.read().split()\n",
    "df2 = dfile2.read().split()\n",
    "df=df1+df2\n",
    "# print(df),''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7090681f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " تجربہ ہندوس آف سپنر روی ایشون نے ایشیاء کپ کی غیر یقینی قسمت اپنی رائے کااظہار کیا پاکستان میں ہونے جارہا\n"
     ]
    }
   ],
   "source": [
    "smdic = open('smdict.txt',encoding='utf-8')\n",
    "smdicstr = smdic.read()\n",
    "tokens  = nltk.word_tokenize(smdicstr)\n",
    "\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = re.sub(r'_', r'', tokens[i])\n",
    "    \n",
    "bgtok = nltk.ngrams(tokens, 2)\n",
    "lbgtok = list(bgtok)\n",
    "fixstr =''\n",
    "ufxstr = ''\n",
    "\n",
    "count = 0\n",
    "\n",
    "while count < len(teststr):\n",
    "    for bgs in range(len(lbgtok)):\n",
    "        for bg in range(len(lbgtok[bgs])):\n",
    "            testingstr = ''\n",
    "            for char in range(len(lbgtok[bgs][bg])):\n",
    "                if char+count < len(teststr):\n",
    "                    testingstr += teststr[char+count]\n",
    "            if testingstr == lbgtok[bgs][bg]:\n",
    "                fixstr += \" \"+ testingstr\n",
    "                count = count+len(lbgtok[bgs][bg])\n",
    "#                 print(lbgtok[bgs][bg])\n",
    "    if len(fixstr) != 0:\n",
    "        ufxstr += (fixstr.split()).pop()\n",
    "    count+=1\n",
    "    \n",
    "# print(ufxstr)\n",
    "print(fixstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c871a",
   "metadata": {
    "id": "142c871a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39611836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smdic = open('Word dictionary/words.txt',encoding='utf-8')\n",
    "# smdicstr = smdic.read()\n",
    "# tokens  = word_tokenize(smdicstr)\n",
    "# teststr = wfile\n",
    "\n",
    "\n",
    "# for i in range(len(tokens)):\n",
    "#     tokens[i] = re.sub(r'_', r'', tokens[i])\n",
    "    \n",
    "# bgtok = nltk.ngrams(tokens, 2)\n",
    "# lbgtok = list(bgtok)\n",
    "# fixstr =''\n",
    "# ufxstr = ''\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# while count < len(teststr):\n",
    "#     for bgs in range(len(lbgtok)):\n",
    "#         for bg in range(len(lbgtok[bgs])):\n",
    "#             testingstr = ''\n",
    "#             for char in range(len(lbgtok[bgs][bg])):\n",
    "#                 testingstr += teststr[char+count]\n",
    "#             if testingstr == lbgtok[bgs][bg]:\n",
    "#                 fixstr += \" \"+ testingstr\n",
    "#                 count = count+len(lbgtok[bgs][bg])\n",
    "#                 print(lbgtok[bgs][bg])\n",
    "#     if len(fixstr) != 0:\n",
    "#         ufxstr += (fixstr.split()).pop()\n",
    "#     count+=1\n",
    "    \n",
    "# print(ufxstr)\n",
    "# print(fixstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "18a30c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alpha\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\alpha\\AppData\\Local\\Temp\\ipykernel_10356\\1424043618.py\", line -1, in <module>\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alpha\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alpha\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alpha\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alpha\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alpha\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alpha\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alpha\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alpha\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alpha\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alpha\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alpha\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alpha\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alpha\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\alpha\\AppData\\Roaming\\Python\\Python311\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "smdic = open('Word dictionary/words.txt',encoding='utf-8')\n",
    "smdicstr = smdic.read()\n",
    "tokens  = nltk.word_tokenize(smdicstr)\n",
    "teststr = wfile\n",
    "\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = re.sub(r'_', r'', tokens[i])\n",
    "    \n",
    "bgtok = nltk.ngrams(tokens, 2)\n",
    "lbgtok = list(bgtok)\n",
    "fixstr =''\n",
    "ufxstr = ''\n",
    "\n",
    "count = 0\n",
    "\n",
    "while count < len(teststr):\n",
    "    for bgs in range(len(lbgtok)):\n",
    "        for bg in range(len(lbgtok[bgs])):\n",
    "            testingstr = ''\n",
    "            for char in range(len(lbgtok[bgs][bg])):\n",
    "                if char+count < len(teststr):\n",
    "                    testingstr += teststr[char+count]\n",
    "            if testingstr == lbgtok[bgs][bg]:\n",
    "                fixstr += \" \"+ testingstr\n",
    "                count = count+len(lbgtok[bgs][bg])\n",
    "#                 print(lbgtok[bgs][bg])\n",
    "    if len(fixstr) != 0:\n",
    "        ufxstr += (fixstr.split()).pop()\n",
    "    count+=1\n",
    "    \n",
    "# print(ufxstr)\n",
    "print(fixstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ed376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
